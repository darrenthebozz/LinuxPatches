diff -ruN linux-6.4.2-clear/arch/x86/mm/fault.c linux-6.4.2/arch/x86/mm/fault.c
--- linux-6.4.2-clear/arch/x86/mm/fault.c	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/arch/x86/mm/fault.c	2023-07-11 16:18:59.943066159 +0000
@@ -1429,7 +1429,7 @@
 					 0, 0, ARCH_DEFAULT_PKEY);
 		return;
 	}
-
+#ifdef CONFIG_OOM
 	if (fault & VM_FAULT_OOM) {
 		/* Kernel mode? Handle exceptions or die: */
 		if (!user_mode(regs)) {
@@ -1446,6 +1446,7 @@
 		 */
 		pagefault_out_of_memory();
 	} else {
+#endif
 		if (fault & (VM_FAULT_SIGBUS|VM_FAULT_HWPOISON|
 			     VM_FAULT_HWPOISON_LARGE))
 			do_sigbus(regs, error_code, address, fault);
@@ -1453,7 +1454,9 @@
 			bad_area_nosemaphore(regs, error_code, address);
 		else
 			BUG();
+#ifdef CONFIG_OOM
 	}
+#endif
 }
 NOKPROBE_SYMBOL(do_user_addr_fault);
 
diff -ruN linux-6.4.2-clear/fs/proc/base.c linux-6.4.2/fs/proc/base.c
--- linux-6.4.2-clear/fs/proc/base.c	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/fs/proc/base.c	2023-07-11 16:18:59.943066159 +0000
@@ -546,7 +546,7 @@
 };
 
 #endif
-
+#ifdef CONFIG_OOM
 static int proc_oom_score(struct seq_file *m, struct pid_namespace *ns,
 			  struct pid *pid, struct task_struct *task)
 {
@@ -567,7 +567,7 @@
 
 	return 0;
 }
-
+#endif
 struct limit_names {
 	const char *name;
 	const char *unit;
@@ -1034,7 +1034,7 @@
 	.llseek		= generic_file_llseek,
 	.release	= mem_release,
 };
-
+#ifdef CONFIG_OOM
 static ssize_t oom_adj_read(struct file *file, char __user *buf, size_t count,
 			    loff_t *ppos)
 {
@@ -1244,7 +1244,7 @@
 	.write		= oom_score_adj_write,
 	.llseek		= default_llseek,
 };
-
+#endif
 #ifdef CONFIG_AUDIT
 #define TMPBUFLEN 11
 static ssize_t proc_loginuid_read(struct file * file, char __user * buf,
@@ -3308,9 +3308,11 @@
 #ifdef CONFIG_PROC_CPU_RESCTRL
 	ONE("cpu_resctrl_groups", S_IRUGO, proc_resctrl_show),
 #endif
+#ifdef CONFIG_OOM
 	ONE("oom_score",  S_IRUGO, proc_oom_score),
 	REG("oom_adj",    S_IRUGO|S_IWUSR, proc_oom_adj_operations),
 	REG("oom_score_adj", S_IRUGO|S_IWUSR, proc_oom_score_adj_operations),
+#endif
 #ifdef CONFIG_AUDIT
 	REG("loginuid",   S_IWUSR|S_IRUGO, proc_loginuid_operations),
 	REG("sessionid",  S_IRUGO, proc_sessionid_operations),
@@ -3656,9 +3658,11 @@
 #ifdef CONFIG_PROC_CPU_RESCTRL
 	ONE("cpu_resctrl_groups", S_IRUGO, proc_resctrl_show),
 #endif
+#ifdef CONFIG_OOM
 	ONE("oom_score", S_IRUGO, proc_oom_score),
 	REG("oom_adj",   S_IRUGO|S_IWUSR, proc_oom_adj_operations),
 	REG("oom_score_adj", S_IRUGO|S_IWUSR, proc_oom_score_adj_operations),
+#endif
 #ifdef CONFIG_AUDIT
 	REG("loginuid",  S_IWUSR|S_IRUGO, proc_loginuid_operations),
 	REG("sessionid",  S_IRUGO, proc_sessionid_operations),
diff -ruN linux-6.4.2-clear/include/linux/oom.h linux-6.4.2/include/linux/oom.h
--- linux-6.4.2-clear/include/linux/oom.h	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/include/linux/oom.h	2023-07-12 13:37:31.653427454 +0000
@@ -10,6 +10,10 @@
 #include <linux/sched/coredump.h> /* MMF_* */
 #include <linux/mm.h> /* VM_FAULT* */
 
+extern struct task_struct *find_lock_task_mm(struct task_struct *p);
+
+#ifdef CONFIG_OOM
+
 struct zonelist;
 struct notifier_block;
 struct mem_cgroup;
@@ -110,6 +114,6 @@
 extern bool oom_killer_disable(signed long timeout);
 extern void oom_killer_enable(void);
 
-extern struct task_struct *find_lock_task_mm(struct task_struct *p);
+#endif /* CONFIG_OOM */
 
 #endif /* _INCLUDE_LINUX_OOM_H */
diff -ruN linux-6.4.2-clear/include/linux/sched/signal.h linux-6.4.2/include/linux/sched/signal.h
--- linux-6.4.2-clear/include/linux/sched/signal.h	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/include/linux/sched/signal.h	2023-07-12 11:58:16.508901062 +0000
@@ -222,7 +222,7 @@
 	unsigned audit_tty;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
-
+#ifdef CONFIG_OOM
 	/*
 	 * Thread is the potential origin of an oom condition; kill first on
 	 * oom
@@ -233,7 +233,7 @@
 					 * Only settable by CAP_SYS_RESOURCE. */
 	struct mm_struct *oom_mm;	/* recorded mm when the thread group got
 					 * killed by the oom killer */
-
+#endif
 	struct mutex cred_guard_mutex;	/* guard against foreign influences on
 					 * credential calculations
 					 * (notably. ptrace)
diff -ruN linux-6.4.2-clear/include/linux/sched.h linux-6.4.2/include/linux/sched.h
--- linux-6.4.2-clear/include/linux/sched.h	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/include/linux/sched.h	2023-07-12 10:50:03.605404460 +0000
@@ -1430,10 +1430,11 @@
 #endif
 
 #ifdef CONFIG_MEMCG
+#ifdef CONFIG_OOM
 	struct mem_cgroup		*memcg_in_oom;
 	gfp_t				memcg_oom_gfp_mask;
 	int				memcg_oom_order;
-
+#endif
 	/* Number of pages to reclaim on returning to userland: */
 	unsigned int			memcg_nr_pages_over_high;
 
@@ -1462,7 +1463,7 @@
 	struct rcu_head			rcu;
 	refcount_t			rcu_users;
 	int				pagefault_disabled;
-#ifdef CONFIG_MMU
+#if defined(CONFIG_MMU) && defined(CONFIG_OOM)
 	struct task_struct		*oom_reaper_list;
 	struct timer_list		oom_reaper_timer;
 #endif
diff -ruN linux-6.4.2-clear/include/trace/events/oom.h linux-6.4.2/include/trace/events/oom.h
--- linux-6.4.2-clear/include/trace/events/oom.h	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/include/trace/events/oom.h	2023-07-11 16:18:59.946399493 +0000
@@ -1,4 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
+#ifdef CONFIG_OOM
 #undef TRACE_SYSTEM
 #define TRACE_SYSTEM oom
 
@@ -193,3 +194,4 @@
 
 /* This part must be outside protection */
 #include <trace/define_trace.h>
+#endif
\ No newline at end of file
diff -ruN linux-6.4.2-clear/kernel/exit.c linux-6.4.2/kernel/exit.c
--- linux-6.4.2-clear/kernel/exit.c	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/kernel/exit.c	2023-07-11 16:18:59.946399493 +0000
@@ -565,8 +565,10 @@
 	mmap_read_unlock(mm);
 	mm_update_next_owner(mm);
 	mmput(mm);
+#ifdef CONFIG_OOM
 	if (test_thread_flag(TIF_MEMDIE))
 		exit_oom_victim();
+#endif
 }
 
 static struct task_struct *find_alive_thread(struct task_struct *p)
diff -ruN linux-6.4.2-clear/kernel/fork.c linux-6.4.2/kernel/fork.c
--- linux-6.4.2-clear/kernel/fork.c	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/kernel/fork.c	2023-07-11 18:52:01.745855035 +0000
@@ -934,7 +934,7 @@
 	free_mm(mm);
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
-
+#ifdef CONFIG_OOM
 static void mmdrop_async_fn(struct work_struct *work)
 {
 	struct mm_struct *mm;
@@ -950,6 +950,7 @@
 		schedule_work(&mm->async_put_work);
 	}
 }
+#endif
 
 static inline void free_signal_struct(struct signal_struct *sig)
 {
@@ -959,8 +960,10 @@
 	 * __mmdrop is not safe to call from softirq context on x86 due to
 	 * pgd_dtor so postpone it to the async context
 	 */
+#ifdef CONFIG_OOM
 	if (sig->oom_mm)
 		mmdrop_async(sig->oom_mm);
+#endif
 	kmem_cache_free(signal_cachep, sig);
 }
 
@@ -1893,10 +1896,10 @@
 
 	tty_audit_fork(sig);
 	sched_autogroup_fork(sig);
-
+#ifdef CONFIG_OOM
 	sig->oom_score_adj = current->signal->oom_score_adj;
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
-
+#endif
 	mutex_init(&sig->cred_guard_mutex);
 	init_rwsem(&sig->exec_update_lock);
 
@@ -2204,6 +2207,7 @@
 		free_task(tsk);
 }
 
+#ifdef CONFIG_OOM
 static void copy_oom_score_adj(u64 clone_flags, struct task_struct *tsk)
 {
 	/* Skip if kernel thread */
@@ -2223,6 +2227,8 @@
 	mutex_unlock(&oom_adj_mutex);
 }
 
+#endif
+
 #ifdef CONFIG_RV
 static void rv_task_fork(struct task_struct *p)
 {
@@ -2735,9 +2741,9 @@
 	trace_task_newtask(p, clone_flags);
 	uprobe_copy_process(p, clone_flags);
 	user_events_fork(p, clone_flags);
-
+#ifdef CONFIG_OOM
 	copy_oom_score_adj(clone_flags, p);
-
+#endif
 	return p;
 
 bad_fork_cancel_cgroup:
diff -ruN linux-6.4.2-clear/mm/Kconfig linux-6.4.2/mm/Kconfig
--- linux-6.4.2-clear/mm/Kconfig	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/mm/Kconfig	2023-07-11 16:18:59.949732826 +0000
@@ -938,6 +938,13 @@
 	  lifetime of the system until these kthreads finish the
 	  initialisation.
 
+config OOM
+	bool "Enable OOM (Out Of Memory) Killer"
+	default y
+	help
+	  OOM Killer tries to recover the system from out of memory cases by
+	  choosing which task lives or dies, freeing up memory.
+
 config PAGE_IDLE_FLAG
 	bool
 	select PAGE_EXTENSION if !64BIT
diff -ruN linux-6.4.2-clear/mm/Makefile linux-6.4.2/mm/Makefile
--- linux-6.4.2-clear/mm/Makefile	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/mm/Makefile	2023-07-11 16:18:59.949732826 +0000
@@ -45,8 +45,7 @@
 ifdef CONFIG_CROSS_MEMORY_ATTACH
 mmu-$(CONFIG_MMU)	+= process_vm_access.o
 endif
-
-obj-y			:= filemap.o mempool.o oom_kill.o fadvise.o \
+obj-y			:= filemap.o mempool.o fadvise.o \
 			   maccess.o page-writeback.o folio-compat.o \
 			   readahead.o swap.o truncate.o vmscan.o shmem.o \
 			   util.o mmzone.o vmstat.o backing-dev.o \
@@ -55,6 +54,10 @@
 			   interval_tree.o list_lru.o workingset.o \
 			   debug.o gup.o mmap_lock.o $(mmu-y)
 
+ifdef CONFIG_OOM
+	obj-y += oom_kill.o
+endif
+
 # Give 'page_alloc' its own module-parameter namespace
 page-alloc-y := page_alloc.o
 page-alloc-$(CONFIG_SHUFFLE_PAGE_ALLOCATOR) += shuffle.o
diff -ruN linux-6.4.2-clear/mm/memory.c linux-6.4.2/mm/memory.c
--- linux-6.4.2-clear/mm/memory.c	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/mm/memory.c	2023-07-11 16:18:59.949732826 +0000
@@ -4068,7 +4068,9 @@
 			update_mmu_tlb(vma, vmf->address, vmf->pte);
 			goto unlock;
 		}
+#ifdef CONFIG_OOM
 		ret = check_stable_address_space(vma->vm_mm);
+#endif
 		if (ret)
 			goto unlock;
 		/* Deliver the page fault to userland, check inside PT lock */
@@ -4108,8 +4110,9 @@
 		update_mmu_tlb(vma, vmf->address, vmf->pte);
 		goto release;
 	}
-
+#ifdef CONFIG_OOM
 	ret = check_stable_address_space(vma->vm_mm);
+#endif
 	if (ret)
 		goto release;
 
@@ -4361,12 +4364,13 @@
 	 * check even for read faults because we might have lost our CoWed
 	 * page
 	 */
+#ifdef CONFIG_OOM
 	if (!(vma->vm_flags & VM_SHARED)) {
 		ret = check_stable_address_space(vma->vm_mm);
 		if (ret)
 			return ret;
 	}
-
+#endif
 	if (pmd_none(*vmf->pmd)) {
 		if (PageTransCompound(page)) {
 			ret = do_set_pmd(vmf, page);
diff -ruN linux-6.4.2-clear/mm/oom_kill.c linux-6.4.2/mm/oom_kill.c
--- linux-6.4.2-clear/mm/oom_kill.c	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/mm/oom_kill.c	2023-07-11 21:13:26.741593248 +0000
@@ -125,31 +125,6 @@
 #endif /* CONFIG_NUMA */
 
 /*
- * The process p may have detached its own ->mm while exiting or through
- * kthread_use_mm(), but one or more of its subthreads may still have a valid
- * pointer.  Return p, or any of its subthreads with a valid ->mm, with
- * task_lock() held.
- */
-struct task_struct *find_lock_task_mm(struct task_struct *p)
-{
-	struct task_struct *t;
-
-	rcu_read_lock();
-
-	for_each_thread(p, t) {
-		task_lock(t);
-		if (likely(t->mm))
-			goto found;
-		task_unlock(t);
-	}
-	t = NULL;
-found:
-	rcu_read_unlock();
-
-	return t;
-}
-
-/*
  * order == -1 means the oom kill is required by sysrq, otherwise only
  * for display purposes.
  */
diff -ruN linux-6.4.2-clear/mm/page_alloc.c linux-6.4.2/mm/page_alloc.c
--- linux-6.4.2-clear/mm/page_alloc.c	2023-07-05 17:30:31.000000000 +0000
+++ linux-6.4.2/mm/page_alloc.c	2023-07-12 13:38:34.753427606 +0000
@@ -114,6 +114,31 @@
 static DEFINE_MUTEX(pcp_batch_high_lock);
 #define MIN_PERCPU_PAGELIST_HIGH_FRACTION (8)
 
+/*
+ * The process p may have detached its own ->mm while exiting or through
+ * kthread_use_mm(), but one or more of its subthreads may still have a valid
+ * pointer.  Return p, or any of its subthreads with a valid ->mm, with
+ * task_lock() held.
+ */
+struct task_struct *find_lock_task_mm(struct task_struct *p)
+{
+    struct task_struct *t;
+
+    rcu_read_lock();
+
+    for_each_thread(p, t) {
+        task_lock(t);
+        if (likely(t->mm))
+            goto found;
+        task_unlock(t);
+    }
+    t = NULL;
+found:
+    rcu_read_unlock();
+
+    return t;
+}
+
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)
 /*
  * On SMP, spin_trylock is sufficient protection.
@@ -2912,9 +2937,10 @@
 			 * failing a high-order atomic allocation in the
 			 * future.
 			 */
+#ifdef CONFIG_OOM
 			if (!page && (alloc_flags & ALLOC_OOM))
 				page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
-
+#endif
 			if (!page) {
 				spin_unlock_irqrestore(&zone->lock, flags);
 				return NULL;
@@ -3204,8 +3230,10 @@
 		 * the exit path shortly and free memory. Any allocation it
 		 * makes during the free path will be small and short-lived.
 		 */
+#ifdef CONFIG_OOM
 		if (alloc_flags & ALLOC_OOM)
 			min -= min / 2;
+#endif
 	}
 
 	/*
@@ -3239,10 +3267,13 @@
 			return true;
 		}
 #endif
+
+#ifdef CONFIG_OOM
 		if ((alloc_flags & (ALLOC_HIGHATOMIC|ALLOC_OOM)) &&
 		    !free_area_empty(area, MIGRATE_HIGHATOMIC)) {
 			return true;
 		}
+#endif
 	}
 	return false;
 }
@@ -3541,10 +3572,12 @@
 	 * contexts that are allowed to allocate outside current's set
 	 * of allowed nodes.
 	 */
+#ifdef CONFIG_OOM
 	if (!(gfp_mask & __GFP_NOMEMALLOC))
 		if (tsk_is_oom_victim(current) ||
 		    (current->flags & (PF_MEMALLOC | PF_EXITING)))
 			filter &= ~SHOW_MEM_FILTER_NODES;
+#endif
 	if (!in_task() || !(gfp_mask & __GFP_DIRECT_RECLAIM))
 		filter &= ~SHOW_MEM_FILTER_NODES;
 
@@ -3595,7 +3628,7 @@
 
 	return page;
 }
-
+#ifdef CONFIG_OOM
 static inline struct page *
 __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,
 	const struct alloc_context *ac, unsigned long *did_some_progress)
@@ -3682,7 +3715,7 @@
 	mutex_unlock(&oom_lock);
 	return page;
 }
-
+#endif
 /*
  * Maximum number of compaction retries with a progress before OOM
  * killer is consider as the only way to move forward.
@@ -3759,9 +3792,10 @@
 	int max_retries = MAX_COMPACT_RETRIES;
 	int min_priority;
 	bool ret = false;
+#ifdef CONFIG_OOM
 	int retries = *compaction_retries;
 	enum compact_priority priority = *compact_priority;
-
+#endif
 	if (!order)
 		return false;
 
@@ -3827,7 +3861,9 @@
 		ret = true;
 	}
 out:
+#ifdef CONFIG_OOM
 	trace_compact_retry(order, priority, compact_result, retries, max_retries, ret);
+#endif
 	return ret;
 }
 #else
@@ -4079,7 +4115,7 @@
 
 	return alloc_flags;
 }
-
+#ifdef CONFIG_OOM
 static bool oom_reserves_allowed(struct task_struct *tsk)
 {
 	if (!tsk_is_oom_victim(tsk))
@@ -4094,7 +4130,7 @@
 
 	return true;
 }
-
+#endif
 /*
  * Distinguish requests which really need access to full memory
  * reserves from oom victims which can live with a portion of it
@@ -4110,8 +4146,10 @@
 	if (!in_interrupt()) {
 		if (current->flags & PF_MEMALLOC)
 			return ALLOC_NO_WATERMARKS;
+#ifdef CONFIG_OOM
 		else if (oom_reserves_allowed(current))
 			return ALLOC_OOM;
+#endif
 	}
 
 	return 0;
@@ -4182,8 +4220,10 @@
 		 */
 		wmark = __zone_watermark_ok(zone, order, min_wmark,
 				ac->highest_zoneidx, alloc_flags, available);
+#ifdef CONFIG_OOM
 		trace_reclaim_retry_zone(z, order, reclaimable,
 				available, min_wmark, *no_progress_loops, wmark);
+#endif
 		if (wmark) {
 			ret = true;
 			break;
@@ -4439,7 +4479,7 @@
 	if (check_retry_cpuset(cpuset_mems_cookie, ac) ||
 	    check_retry_zonelist(zonelist_iter_cookie))
 		goto restart;
-
+#ifdef CONFIG_OOM
 	/* Reclaim has failed us, start killing things */
 	page = __alloc_pages_may_oom(gfp_mask, order, ac, &did_some_progress);
 	if (page)
@@ -4450,7 +4490,7 @@
 	    (alloc_flags & ALLOC_OOM ||
 	     (gfp_mask & __GFP_NOMEMALLOC)))
 		goto nopage;
-
+#endif
 	/* Retry as long as the OOM killer is making progress */
 	if (did_some_progress) {
 		no_progress_loops = 0;
